{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrkmsobHk1wbMY1f9tiBGQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhananjai14/LLM_tutorials/blob/main/LangChain_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. LangChain\n",
        "\n",
        "New to Gen AI Application development, Langchain would be of great help to you.\n",
        "\n",
        "So what is **Langchain**?\n",
        "\n",
        "LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
        "\n",
        "\n",
        "Langchain provide and easy functionality to add features like buffer memory or agents or prompts template.\n",
        "\n",
        "\n",
        "Entire document is divided into following section:\n",
        "1. Installation\n",
        "2. How to load LLM in Langchain\n",
        "3. Prompt Template\n",
        "4. Chains\n",
        "5. Agents and Tools\n",
        "6. Memory\n",
        "7. Document Loader\n",
        "\n",
        "LangChain documentation:\n",
        "\n",
        "V0.2 -> https://python.langchain.com/v0.2/docs/introduction/\n",
        "\n",
        "V0.1 -> https://python.langchain.com/v0.1/docs/get_started/introduction/\n",
        "\n",
        "\n",
        "For GitHub: [Click Here](https://github.com/langchain-ai/langchain)\n",
        "\n"
      ],
      "metadata": {
        "id": "OsnPKhWEZrog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Installation"
      ],
      "metadata": {
        "id": "WCFsy5buPHMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install langchain-community -q\n",
        "!pip install openai -q\n",
        "!pip install  langchain-google-genai -q"
      ],
      "metadata": {
        "id": "r8p70H8bZy3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4564bbc-289d-4612-e6b8-ffc4826b1d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "import os\n"
      ],
      "metadata": {
        "id": "Iu9QOUYZJIxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Load LLM in LangChain\n",
        "Aim is to connect various APIs with LangChain\n",
        "1. OpenAI API using langchain\n",
        "2. Huggingface API using LangChain\n",
        "3. Gemini API using langchain\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VtCBPWmPZDxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI API using langchain"
      ],
      "metadata": {
        "id": "z15kmaFSP_RX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.llms import OpenAI\n",
        "from langchain_community.llms import OpenAI\n",
        "import openai\n"
      ],
      "metadata": {
        "id": "ZUODWTzhZMwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "uYsVHX8rddAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(api_key=OPENAI_API_KEY, temperature = .9)\n",
        "# Temperature means creativity in responce. Its value ranges from (0,1)\n",
        "# 0 - No creativity, 1- very creative"
      ],
      "metadata": {
        "id": "r0DxndYFdJkT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e1d995-bf4c-45a8-a5c0-f9980e46d13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Which is most creative company in india?'\n",
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBZmULhSaC4I",
        "outputId": "ca37cdc4-1afe-41c0-e0f7-6455dd9760e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There are many creative companies in India, each specializing in different industries and areas of creativity. Some of the most notable ones include:\n",
            "\n",
            "1. Tata Group - Known for their innovative thinking and diverse portfolio of businesses, Tata Group is considered one of the most creative companies in India.\n",
            "\n",
            "2. Infosys - This global IT services and consulting company has a reputation for its creativity in delivering solutions to its clients.\n",
            "\n",
            "3. Mahindra & Mahindra - With its focus on sustainability and innovation, Mahindra & Mahindra is a top creative company in the automotive industry.\n",
            "\n",
            "4. Zomato - This food delivery and restaurant discovery platform is known for its creative marketing campaigns and innovative business strategies.\n",
            "\n",
            "5. Ola - As India's largest ride-hailing service, Ola is constantly coming up with new and creative ways to improve its services and expand its reach.\n",
            "\n",
            "6. Mindtree - A leading digital transformation and technology consulting company, Mindtree is recognized for its innovative solutions and creative approach to problem-solving.\n",
            "\n",
            "7. Swiggy - Another popular food delivery platform in India, Swiggy is known for its creative marketing campaigns and unique features such as \"Swiggy Pop\" and \"Swiggy Super.\"\n",
            "\n",
            "8. BigBasket - As one of the pioneers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Langchain with HuggingFace API"
      ],
      "metadata": {
        "id": "F8DGVwCbZPTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "# from langchain import HuggingFaceHub # Depreated version\n",
        "from langchain_community.llms.huggingface_hub import HuggingFaceHub"
      ],
      "metadata": {
        "id": "yj6NoPxoZPca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HugginFace_token=userdata.get('HugginFace_token')"
      ],
      "metadata": {
        "id": "EGS00-NWfmDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemma_model = HuggingFaceHub(repo_id='google/gemma-7b',\n",
        "                             huggingfacehub_api_token=HugginFace_token,\n",
        "                             model_kwargs={'temperature': .5, 'max_length': 64})\n",
        "\n",
        "print(gemma_model('Translate follow statement \"how old are you\" in hindi'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NNZaTFsezP0",
        "outputId": "3d59a2bd-4c28-48b4-e45c-393e3567d5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate follow statement \"how old are you\" in hindi\n",
            "\n",
            "Answer:  \n",
            "\n",
            "Step 1/2\n",
            "\"How old are you\" in Hindi is translated as:\n",
            "\n",
            "Step 2/2\n",
            "\"कितने वर्ष के हो आप\" (Kitne vars ke ho aap)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Langchain with Gemini API"
      ],
      "metadata": {
        "id": "Ni79B0yhZPme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "GeminiAPI_key = userdata.get('GeminiAPI_key')"
      ],
      "metadata": {
        "id": "A09d0UNik2V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_model = ChatGoogleGenerativeAI(model = 'gemini-pro', google_api_key=GeminiAPI_key)\n"
      ],
      "metadata": {
        "id": "zUPuqZktk2hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gemini_model.invoke('Till what date your knowledge base is created?').content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K03DWs9Bk2tc",
        "outputId": "3dad61d7-15aa-4bae-95bb-6ca40de487d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My knowledge cutoff is April 2023. This means that I do not have access to real-time information or events that occurred after this date. For the most up-to-date information, please consult a reliable and up-to-date source.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdMYbwASN5xN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Prompt Template"
      ],
      "metadata": {
        "id": "7s7p_U0TgZ2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template_name = PromptTemplate.from_template(template = 'I want to open a {restaurant_type} restaurant in India. Suggsest me a creative name.')\n",
        "prompt = prompt_template_name.format(restaurant_type = 'Indian')\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rylPth-ngcyu",
        "outputId": "b47fd870-a281-41df-9f3a-ecb199f0b1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a Indian restaurant in India. Suggsest me a creative name.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template_name = PromptTemplate.from_template(template = 'Which is the best place or state to visit in {country} durring {season_type} season.')\n",
        "prompt = prompt_template_name.format(country = 'India', season_type = 'summer')\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUkRax-TmI6L",
        "outputId": "4dad1c17-3e9d-4283-bdae-6050916af277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which is the best place or state to visit in India durring summer season.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like wise any prompts can be created as per the requirements."
      ],
      "metadata": {
        "id": "VY5PwyK0nJuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Chains\n",
        "\n",
        "Now to use prompt template with LLM, we have to use **Chains.**\n",
        "\n",
        "Steps to follow:\n",
        "1. Initialize the LLM\n",
        "2. Create the prompt template"
      ],
      "metadata": {
        "id": "TdfLJTh1nbG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(api_key=OPENAI_API_KEY, temperature = .9)\n",
        "prompt_template_name = PromptTemplate.from_template(template = 'Which is the best place or state to visit in {country} durring {season_type} season.')\n",
        "prompt_template_name.format(country = 'India', season_type = 'summer')\n"
      ],
      "metadata": {
        "id": "DWEJkWj6n_42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "90438916-b8c2-43e7-8670-b7e5778eba00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Which is the best place or state to visit in India durring summer season.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm = llm, prompt = prompt_template_name, verbose = True)\n",
        "\n",
        "response = chain.run(country = 'India', season_type = 'summer')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnRpSbgxufWm",
        "outputId": "61df93e0-24c0-4824-ef57-d47fee71408c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWhich is the best place or state to visit in India durring summer season.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "One of the best places to visit in India during the summer season is Ladakh, located in the state of Jammu and Kashmir. It is known for its stunning landscapes, including snow-capped mountains, serene lakes, and beautiful valleys. The weather in Ladakh is pleasantly cool during the summer months, making it an ideal escape from the scorching heat in other parts of India. Other popular places to visit during summer in India include Shimla in Himachal Pradesh, Munnar in Kerala, and Coorg in Karnataka.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let say there is requirements to multiple prompt template in that case you will use **Simple Sequential Chain**"
      ],
      "metadata": {
        "id": "tthR2zAIukBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(api_key = OPENAI_API_KEY, temperature = .6)\n",
        "\n",
        "#Prompt 1\n",
        "prompt_template1 = PromptTemplate.from_template(template = 'I want to open a restaurant for {cusine} food. Suggsest me a creative name.')\n",
        "\n",
        "chain1 = LLMChain(llm = llm, prompt = prompt_template1, output_key = 'restaurant_name')\n",
        "\n",
        "# Prompt 2\n",
        "prompt_template2 = PromptTemplate.from_template(template = 'Suggest some menu items for {restaurant_name} food.')\n",
        "\n",
        "chain2 = LLMChain(llm = llm, prompt = prompt_template2, output_key = 'menu_items')\n"
      ],
      "metadata": {
        "id": "tUfvfpMFL3sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the chains\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains = [chain1,chain2])\n"
      ],
      "metadata": {
        "id": "fZ6VE0_dOdCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = chain.run('Indian')\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hO3JGcIPDL3",
        "outputId": "6f09d486-d3d0-487e-ba4f-b8ee4e1d934a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. \"Spice Garden\" Menu Items:\n",
            "- Spiced Vegetable Samosas\n",
            "- Tandoori Chicken Skewers\n",
            "- Vegetable Biryani\n",
            "- Butter Chicken\n",
            "- Aloo Gobi (potato and cauliflower curry)\n",
            "- Lamb Vindaloo\n",
            "- Garlic Naan\n",
            "- Mango Lassi (yogurt drink)\n",
            "- Gulab Jamun (sweet milk balls in syrup)\n",
            "- Chai Tea\n",
            "\n",
            "2. \"Tandoori Tales\" Menu Items:\n",
            "- Tandoori Shrimp\n",
            "- Chicken Tikka Masala\n",
            "- Lamb Kebabs\n",
            "- Palak Paneer (spinach and cheese curry)\n",
            "- Vegetable Jalfrezi\n",
            "- Tandoori Roti\n",
            "- Mango Chutney\n",
            "- Raita (yogurt dip)\n",
            "- Kulfi (Indian ice cream)\n",
            "- Masala Chai\n",
            "\n",
            "3. \"Curry Kingdom\" Menu Items:\n",
            "- Chicken Curry\n",
            "- Beef Korma\n",
            "- Dal Makhani (creamy lentil dish)\n",
            "- Aloo Tikki (potato fritters)\n",
            "- Saag Paneer (spinach and cheese curry)\n",
            "- Garlic Naan\n",
            "- Cucumber Raita\n",
            "- Mango Lassi\n",
            "- Gajar Halwa (carrot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential Chain** It gives the output of both the chain  "
      ],
      "metadata": {
        "id": "yZ647XecPTEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the chains\n",
        "from langchain.chains import SequentialChain\n",
        "chain = SequentialChain(chains = [chain1,chain2]\n",
        "                        , input_variables = ['cusine']\n",
        "                        , output_variables = ['restaurant_name', 'menu_items'])\n",
        "\n"
      ],
      "metadata": {
        "id": "b_s-PrlBP7Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain({'cusine':'Indian'})\n",
        "print(response['cusine'])\n",
        "print(response['restaurant_name'])\n",
        "print(response['menu_items'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uusEIiO3UewG",
        "outputId": "3da4fa74-71c1-4935-a1fe-8cb99f8421c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indian\n",
            "\n",
            "\n",
            "\"Masala Magic\"\n",
            "\n",
            "1. Butter Chicken Masala\n",
            "2. Vegetable Biryani\n",
            "3. Tandoori Chicken\n",
            "4. Aloo Gobi (potatoes and cauliflower in a spicy tomato-based sauce)\n",
            "5. Paneer Tikka Masala (grilled cottage cheese in a creamy tomato sauce)\n",
            "6. Chicken Tikka Masala\n",
            "7. Palak Paneer (spinach and cottage cheese curry)\n",
            "8. Lamb Rogan Josh (slow-cooked lamb in a spicy gravy)\n",
            "9. Chana Masala (chickpea curry)\n",
            "10. Naan bread (plain or stuffed with garlic or cheese)\n",
            "11. Samosas (crispy pastry filled with spiced potatoes and peas)\n",
            "12. Mango Lassi (yogurt drink with mango puree)\n",
            "13. Gulab Jamun (deep-fried dough balls soaked in sugar syrup)\n",
            "14. Masala Chai (spiced tea)\n",
            "15. Papadum (thin and crispy lentil crackers)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Agents and Tools\n",
        "\n",
        "The knowledge base of any LLM model is fixed. For example ChatGPT has knowledege base limited to Jan 2022 and any information asked beyond that, ChatGPT is not able to answer. So the solution is\n",
        "1. Train the model on new data\n",
        "2. RAG Approach\n",
        "3. Use real time search engine.\n",
        "\n",
        "Here we be calling **Agenta** to perform this task. SO agents will fetch the data in real time and LLM will process it and show the result to user.  "
      ],
      "metadata": {
        "id": "MEGtWHJtVJfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X60ouRUbVji6",
        "outputId": "34209389-3fe6-4b15-89c1-aefc5808f3ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a free account at SerpApi"
      ],
      "metadata": {
        "id": "J1pk8ajMVcvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "SERPAPI_KEY = userdata.get('SERPAPI_KEY')"
      ],
      "metadata": {
        "id": "dXmIw9BVclJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n"
      ],
      "metadata": {
        "id": "UKQOlzROcx3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(api_key=OPENAI_API_KEY, temperature = .2)\n",
        "\n",
        "# Google search API\n",
        "tools = load_tools(['serpapi', 'llm-math'],serpapi_api_key = SERPAPI_KEY,llm=llm)\n",
        "\n",
        "# initialize the agents\n",
        "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True)\n",
        "\n",
        "agent.run('What was the GDP of US in 2023')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "sElIWI3odqog",
        "outputId": "0c1fd505-51db-4648-eb5c-fc0050b092a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should search for the GDP of US in 2023\n",
            "Action: Search\n",
            "Action Input: \"GDP of US in 2023\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mabout 27.36 trillion U.S. dollars\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use a calculator to convert the result to a more readable format\n",
            "Action: Calculator\n",
            "Action Input: 27.36 trillion U.S. dollars\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 27360000000000\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should add commas to make the number easier to read\n",
            "Action: Calculator\n",
            "Action Input: 27360000000000\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 27360000000000\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The GDP of US in 2023 was 27.36 trillion U.S. dollars.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The GDP of US in 2023 was 27.36 trillion U.S. dollars.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Memory\n",
        "\n",
        "Let say if we want our LLM to retain the context of previous chat, then we need to intorduce **Mempory** in LangChain.\n"
      ],
      "metadata": {
        "id": "yRJCrwKyfAxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(api_key=OPENAI_API_KEY, temperature = .2)\n",
        "\n",
        "# Creating the prompts\n",
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = PromptTemplate(input_variables = ['cusine'],\n",
        "                                 template = 'I want to open a restaurant for {cusine} food. Suggest a fancy name.')\n",
        "\n",
        "\n",
        "# Creating the chains\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm = llm, prompt = prompt_template  )\n",
        "name = chain.run('Mexican')\n",
        "print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_Tq9-LZkN3r",
        "outputId": "d764eb3d-d9b0-4437-f86f-aafeefdffd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"El Sabor de México\" (The Flavor of Mexico)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm = llm, prompt = prompt_template  )\n",
        "name = chain.run('Indian')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ4j0C1km6sB",
        "outputId": "d1f535eb-e8ea-46fa-b016-1b3236424de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Spice Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-Xk1LT8nBnT",
        "outputId": "05eb1409-889a-4226-90ea-131809702e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the LLM is not able to store the information of past conversation. To over come this we will use **ConversationBufferMemory**"
      ],
      "metadata": {
        "id": "gbySCJZdnOQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm =llm, prompt = prompt_template, memory=memory)\n",
        "name = chain.run('Mexican')\n",
        "print(name)\n",
        "\n",
        "chain = LLMChain(llm =llm, prompt = prompt_template, memory=memory)\n",
        "name = chain.run('Indian')\n",
        "print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZpuEB6-nd0S",
        "outputId": "22f040a2-0ebd-47c7-e6cb-af721879bdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"El Sabor de México\" (The Flavor of Mexico)\n",
            "\n",
            "\n",
            "\"Maharaja's Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3pddgvhoFXw",
        "outputId": "3b2c2028-6324-40cb-b29d-0f51bfcccc17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Mexican\n",
            "AI: \n",
            "\n",
            "\"El Sabor de México\" (The Flavor of Mexico)\n",
            "Human: Indian\n",
            "AI: \n",
            "\n",
            "\"Maharaja's Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem with ConversationBufferMemory is that it stores every conversation in memory, that consumes a lot of memory.\n",
        "\n",
        "To overcome this issue, we have **ConversationBufferWindowMemory**, it takes in the parameter that mentions how many conversation LLM need to remember.\n"
      ],
      "metadata": {
        "id": "jDCK50vAoVnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "chain = LLMChain(llm =llm, prompt = prompt_template, memory=memory)\n",
        "name = chain.run('Mexican')\n",
        "print(name)\n",
        "\n",
        "chain = LLMChain(llm =llm, prompt = prompt_template, memory=memory)\n",
        "name = chain.run('Indian')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaArw2n0p4cL",
        "outputId": "a2a1bcf2-943c-49ce-f8fa-05175f41ae54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"El Sabor de México\" (The Flavor of Mexico)\n",
            "\n",
            "\n",
            "\"Spice Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory.buffer)\n",
        "# Since k = 1, the LLM remembers only one conversation."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPDEPmCVqOqY",
        "outputId": "f270b4fa-63a8-4de0-9fdf-e858b2720381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Indian\n",
            "AI: \n",
            "\n",
            "\"Spice Palace\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Document Loader\n",
        "\n",
        "To upload any document this functionality is used.\n",
        "Official documentation for document loader can be found [here.](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)\n"
      ],
      "metadata": {
        "id": "-1cKpjzQqbag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzph_hOkuzc2",
        "outputId": "de168a9b-ff42-4159-9629-f50461de7107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/295.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/TRANSFORMERS FOR DEEP LEARNING TASKS.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "BIG0vvuxtbcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTJQI_ILvcL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}